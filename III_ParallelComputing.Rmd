---
title: "Assignment III: Parallel computing"
author: "Submitted by Tim-Moritz Bündert"
date: "July 04, 2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


I hereby assure that my submission is in line with the *Examination and Assessment Honor Code* outlined on the lecture slides and below:

### Examination and Assessment Honor Code

All members of the School of Business and Economics at the University of Tübingen (faculty, students, and alumni) share a commitment to honesty and integrity. In particular, all members follow the standards of scholarship and professionalism in all examinations and assessments.

By submitting these assignments, students agree to comply with this Examination and Assessment Honor Code.

Students who violate this Honor Code are in breach of this agreement and are subject to sanctions imposed by the School of Business and Economics, the University and its responsible bodies (e.g., the board of examiners (“Prüfungsausschuss”)).

1. All members of the School of Business and Economics at the University of Tübingen (faculty, students, and alumni) have the obligation to report known violations to the responsible bodies (e.g., the board of examiners (“Prüfungsausschuss”) or the Dean of Programs)
2. You must not represent another’s work as your own
3. You must not receive inadmissible assistance of any sort before, during, or after an examination or other forms of course work that is subject to assessment by faculty members
4. You must not provide inadmissible assistance of any sort before, during, or after an examination or other forms of course work that is subject to assessment by faculty members
5. Violations of this Honor Code will be handled according to the rules and regulations laid out in the rules for this course


## General setup

Before I start the project, I install (if necessary) and load the packages that are needed for the assignment.

```{r script_header, message=FALSE, warning = FALSE}
# Check if packages have been installed before; if not, install them
if (!require("MASS")) install.packages("MASS"); library(MASS)
if (!require("Matrix")) install.packages("Matrix"); library(Matrix)
if (!require("glmnet")) install.packages("glmnet"); library(glmnet)
if (!require("randomForest")) install.packages("randomForest"); library(randomForest)
if (!require("xgboost")) install.packages("xgboost"); library(xgboost)
if (!require("parallel")) install.packages("parallel"); library(parallel)
if (!require("foreach")) install.packages("foreach"); library(foreach)
if (!require("doParallel")) install.packages("doParallel"); library(doParallel)
if (!require("future")) install.packages("future"); library(future)
if (!require("future.apply")) install.packages("future.apply"); library(future.apply)
if (!require("doRNG")) install.packages("doRNG"); library(doRNG)
if (!require("data.table")) install.packages("data.table"); library(data.table)
if (!require("bigmemory")) install.packages("bigmemory"); library(bigmemory)
if (!require("biganalytics")) install.packages("biganalytics"); library(biganalytics)
if (!require("profvis")) install.packages("profvis"); library(profvis)
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("fixest")) install.packages("fixest"); library(fixest)
if (!require("itertools")) install.packages("itertools"); library(itertools)
```

Next, I make my hardware and software specs transparent:

```{r specs}
sessionInfo()
Sys.info()                              # Operating system
parallel::detectCores(logical = FALSE)  # CPU cores
benchmarkme::get_ram()                  # RAM
```

# Exercise 1: Comparing different prediction methods on simulated data
> Your task now is to compare the predictive accuracy of different (machine learning) methods on 100 different simulated datasets generated by that function.

First, the function `generate_data()` is sourced such that it can be subsequently used.

```{r 1_source, echo=TRUE}
source("generate_data.R")
```

As the next step, a function is defined which executes the desired workflow and can be called repetitively in the following. There, a matrix containing the respective MSE results is initialized and the data for the iteration is generated by calling the previously sourced function `generate_data()`. Then, the classifiers (linear and lasso regression, random forest, XGBoost) are trained on the training and evaluated using the test set. Finally, the MSE results are stored in the data frame which will be returned.

```{r 1_function, echo=TRUE}
ML_predictions <- function(trial) {
  MSE_matrix <- data.frame(lm = double(1),
                           glmnet = double(1),
                           randomForest = double(1),
                           xgboost = double(1))
  
  data <- generate_data()
  train_data <- data$train
  test_data <- data$test
    
  # lm
  m_lm <- lm(outcome ~ ., train_data)
  p_lm <- predict(m_lm, newdata = test_data) 
  mse_lm <- mean((p_lm - test_data$outcome)^2)
  MSE_matrix$lm[1] <- mse_lm
  
  # glmnet
  m_glm <- glmnet(x = as.matrix(train_data[-1]), y = train_data$outcome)
  p_glm <- predict(m_glm, newx = as.matrix(test_data[-1])) 
  mse_glm <- mean((p_glm - test_data$outcome)^2)
  MSE_matrix$glmnet[1] <- mse_glm
    
  # randomForest
  m_rf <- randomForest(outcome ~ ., data = train_data)
  p_rf <- predict(m_rf, newdata = test_data) 
  mse_rf <- mean((p_rf - test_data$outcome)^2)
  MSE_matrix$randomForest[1] <- mse_rf
    
  # xgboost
  data_xgb = xgb.DMatrix(data = as.matrix(train_data[-1]), label = train_data$outcome)
  m_xgb <- xgboost(data = data_xgb, nrounds = 100, verbose = F)
  p_xgb <- predict(m_xgb, newdata = as.matrix(test_data[-1])) 
  mse_xgb <- mean((p_xgb - test_data$outcome)^2)
  MSE_matrix$xgboost[1] <- mse_xgb
  
  return(as.matrix(MSE_matrix))
}
```

In addition, the number of trials for which this function should be executed is defined (`trials`).

```{r 1_trials, echo=TRUE}
trials <- 1:100
```

## 1. Sequentially

Using the previously defined function `ML_predictions()`, the workflow is executed sequentially 100 times using `sapply()`. Essentially, this operation could also be implemented by looping through the algorithm 100 times, however, using the function is more convenient and also needed for the upcoming approaches. To ensure reproducibility, the function `set.seed()` is implemented and the means of the methods across the 100 trials are printed.

```{r 1_sequentially, echo=TRUE}
system.time({
  set.seed(42)
  res_seq <- sapply(trials, ML_predictions)
})

print(rowMeans(res_seq))
```

Hence, it can be seen that the ensemble methods random forest and XGBoost perform significantly better than the linear and lasso regression method.

## 2. In parallel, using the `{parallel}` framework

Next, the workflow is implemented using `{parallel}`. For that, a cluster is initialized using all available cores (on this machine: 8). Next, it is ensured that the results are reproducible by setting `clusterSetRNGStream()` on this created cluster. Next, the R-packages and functions necessary for the workflow are exported to the workers. Finally, `parallel::parSapply()` is used to implement the workflow for the pre-specified number of trials (100) in parallel. Again, the average MSEs are printed.

```{r 1_parallel, echo=TRUE}
system.time({
  cls <- makeCluster(detectCores())
  clusterSetRNGStream(cls, iseed = 42)
  
  clusterEvalQ(cls, {
    library(glmnet) 
    library(randomForest) 
    library(xgboost)
    })
  
  # export functions
  clusterExport(cls, c("generate_data", "ML_predictions")) 
  
  results <- parSapply(cls, trials, ML_predictions)
  stopCluster(cls)
})

print(rowMeans(results))
```

A similar result to the sequential approach can be identified where the ensemble methods lead to significantly better results than the regression techniques. 

As second variant of using `{parallel}`, `parallel::mcmapply()` can be used on MacOs/Linux which offers the advantage of "forking" the system call, so the individual processes inherit from the current process. Therefore, it not not needed to copy data or explicitly initialize a cluster. This might result in lower overhead. Again, it is ensured that the results are reproducible by setting the argument `mc.set.seed`.

```{r 1_parallel2, echo=TRUE}
system.time({
  results <- mcmapply(trials, FUN = ML_predictions, mc.cores = detectCores(), mc.set.seed = 42)
})

print(rowMeans(results))
```

Again, a similar result is obtained.

## 3. In parallel, using the `{foreach}` framework

Next, the same operation is implemented using `{foreach}`. For this purpose, an implicit cluster with all available cores is initialized and a random seed is set to ensure reproducibility. Then, `foreach::foreach()` is used to execute the workflow the desired number of times in parallel while specifying the way of combining results (`rbind`) and the required packages are exported.

Note that this package could also be used to implement the sequential approach by replacing `%dopar%` with `%do%`. Also, `%dorng%` from `{doRNG}` could be used to run the method in parallel and set the random number generator by adding `set.seed()` beforehand.

```{r 1_foreach, echo=TRUE}
system.time({
  registerDoParallel(detectCores())
  registerDoRNG(123) 
  result <- foreach(trial = trials, .combine = rbind, .packages = c("glmnet", "randomForest", "xgboost")) %dopar% ML_predictions(trial)
  stopImplicitCluster()
})

colMeans(result)
```

Hence, similar results to the ones obtained in the previous approaches can be observed.

As an add-on, it is investigated whether chunking can avoid significant overhead and thereby reduce computation times. Therefore, the trials are split into as many parts as there are cores (workers). Apart from that, nothing changes.

```{r 1_foreach2, echo=TRUE}
system.time({
  registerDoParallel(detectCores())
  registerDoRNG(123) 
  result <- foreach(trial =  isplitVector(trials, chunks = detectCores()), .combine = rbind, .packages = c("glmnet", "randomForest", "xgboost")) %dopar% ML_predictions(trial)
  stopImplicitCluster()
})

colMeans(result)
```

Hence, it can be seen that chunking significantly reduces the computation times while leading to similar results.

## 4. In parallel, using the `{future}` framework

Finally, the same operation is executed using the `{future}` framework. This is done by setting a multisession plan which means that the values are computed in parallel R sessions. Then, the apply variant of this framework (`future_apply::future_sapply()`) is called for the pre-specified number of times on the function `ML_predictions()`. With the option `future.seed`, it can be ensured that the results are reproducible. Finally, a sequential plan is set to shut down the parallel workers.

```{r 1_future, echo=TRUE}
system.time({
  plan(multisession)
  x <- future_sapply(trials, ML_predictions, future.seed = 43)
  plan(sequential)
})

rowMeans(x)
```

As it was the case for the previous methods, it can be observed that the ensemble methods lead on average to lower MSEs than the two regression techniques.

## Comparison of execution times & reflection on `{future}`

As all of the implemented approaches yield similar results (not exact due to randomness), it is reasonable to compare the respective execution times.

*Note: the exact execution times might slightly deviate, the presented ones are from a previous run.*

Approach | Time |
-------------------- | -------------
Sequential computing | 179.9 sec. 
`parallel::parSapply()` | 39.7 sec.
`parallel::mcmapply()` | 34.8 sec.
`{foreach}` - base | 34.3 sec.
`{foreach}` - chunking | **2.8 sec.**
`{future}` | 45.8 sec 


Here, it can be seen that all parallel approaches implement the specified workflow significantly faster than doing so sequentially with the `{foreach}` variant being the fastest framework. Apparently, the increased overhead due to the communication with the workers is outweighed by the benefit of making use of all cores, in this use case. In addition, the chunking-based method reduces the computation time again significantly compared to "basic" parallel approaches where one trial is executed per worker. By directing more trials to the workers (i.e. splitting the trials into as many equal parts as there are workers), the overhead of communication between workers and master can be avoided which results in an improvement of speed.

> Briefly (2-3 sentences) comment on your experience using `{future}`. What did you like about it, what didn’t you like? Where do you see benefits compared to the other frameworks?

As an alternative way to implement parallel computations in R, `{future}` provides the benefit that there is no need for code modifications when switching the backend e.g., from sequential to parallel. Also, this switching is very easy as one needs only to adapt the plan. This is similar to `foreach()`, however, what is really convenient is that `{future}` automatically exports all variables, functions and packages to the workers. In addition, reproducibility can be easily implemented in parallel by specifying the option `future.seed`. On the downside, this approach performs the discussed operation roughly ten seconds slower than `foreach()` which might scale for significantly larger tasks.


# Exercise 2: Multiple regressions in parallel using shared memory
> Compare 4 different approaches both in terms of memory consumption and speed

In the following, four approaches for conducting multiple regressions will be implemented and compared.

## 1. Sequential computing from RAM 

In the first approach, the entire dataset is loaded into RAM using `data.table::fread()` and selecting only the variables of interest. Subsequently, the task under consideration is formulated as a function by specifying the different formulas and a function which returns the desired coefficient of a trained regression model. Here, `lm()` is used to estimate the regression whereas `fixest:feols()` or `lfe::felm()` would have been other possibilities. However, the baseline function is chosen here to determine the potential improvements by using parallel computing and/or shared memory.

```{r 2_seqRAM, echo=TRUE}
flights <- fread("Data.nosync/flights_2013_05.csv", 
                  select = c("ArrDelay", "DepDelay", "Distance", "DayOfWeek", 
                             "DayofMonth", "Quarter", "OriginState"))
  
form1 <- formula(ArrDelay ~ DepDelay + Distance + DayOfWeek + DayofMonth + Quarter + as.factor(OriginState))
form2 <- formula(ArrDelay ~ DepDelay + DayOfWeek + DayofMonth + Quarter + as.factor(OriginState))
form3 <- formula(ArrDelay ~ DepDelay + Distance + DayofMonth + Quarter + as.factor(OriginState))
form4 <- formula(ArrDelay ~ DepDelay + Distance + DayOfWeek + Quarter)
  
lm_coef <- function(form) {coef(lm(form, flights))["DepDelay"]}
forms <- list(form1, form2, form3, form4)
```

The task can be implemented sequentially by using `sapply()` to implement `lm_coef()` for each of the formulas in `forms`. To measure the speed and memory consumption, `system.time()` and `Rprof()` are used. This is because `{profvis}` only monitors the master process and not accounts for the workers. Hence, to ensure accuracy and a comparison of parallel and sequential approaches, this way of profiling is chosen. The memory consumption is returned in kilobytes (kB).

```{r 2_seqRAM2, echo=TRUE}
system.time({
  Rprof("Rprof_table_seqRAM.out", memory.profiling = TRUE)
  res_RAM_seq <- sapply(forms, lm_coef)
  Rprof(NULL)
})
summary_out <- summaryRprof("Rprof_table_seqRAM.out", memory = "tseries", diff = FALSE)
summary_out$mem.used <- (summary_out$vsize.small + summary_out$vsize.large) / 1024
print(max(summary_out$mem.used) - min(summary_out$mem.used))
```

## 2. Parallel computing from RAM

The same task can be implemented using `{foreach}` as the parallel framework. As in the first exercise, the workers are initialized on all available cores and `foreach::foreach()` is used to apply `lm_coef()` to each formula in **forms**. Finally, the created cluster is stopped. 

*Note: for the two parallel approaches (2, 4), the measurement of memory consumption does not work properly when knitting the HTML-document (either showing 0 or throwing an error). Therefore, these chunks are not evaluated in knitted HTML-file, but the measurements of previous runs in the R-Markdown file are given where everything works well. The results can be reproduced by running the respective chunks in the R-Markdown file.*

```{r 2_parRAM, echo=TRUE}
registerDoParallel(detectCores())
system.time({
  res_RAM_fe <- foreach(form = forms, combine = c) %dopar% lm_coef(form)
})
stopImplicitCluster()
```

```{r 2_parRAM2, eval=FALSE, include=TRUE}
registerDoParallel(detectCores())

Rprof("Rprof_table_parRAM.out", memory.profiling = TRUE)
res_RAM_fe <- foreach(form = forms, combine = c) %dopar% lm_coef(form)
Rprof(NULL)
summary_out <- summaryRprof("Rprof_table_parRAM.out", memory = "tseries", diff = FALSE)
summary_out$mem.used <- (summary_out$vsize.small + summary_out$vsize.large) / 1024
print(max(summary_out$mem.used) - min(summary_out$mem.used))

stopImplicitCluster()

# run in Markdown: 854.5 kB
```

## 3. Sequential computing with a `big.matrix`

In the third approach, the task is implemented using the `{bigmemory}` framework which allows to process massive datasets. For this purpose, the dataset is saved as a `big.matrix`.

```{r 2_seqBM, eval=FALSE, include=TRUE}
flights_bm <- as.big.matrix(flights, backingfile = "flights_num.bin", descriptorfile = "flights_num.desc", 
                            backingpath = paste0(getwd(),"/Data.nosync"))
```

Hence, this matrix can now be easily accessed using the filebacking. As now `biganalytics::biglm.big.matrix()` is used to perform the regression, the formulas need to be slightly adapted to include *OriginState* as a factor variable. Apart from that, the idea is similar as before: a function is defined which returns the desired coefficient for a given formula and now, factor variable.

```{r 2_seqBM2, echo=TRUE}
flights_bm <- attach.big.matrix("Data.nosync/flights_num.desc")

form1 <- formula(ArrDelay ~ DepDelay + Distance + DayOfWeek + DayofMonth + Quarter + OriginState)
form2 <- formula(ArrDelay ~ DepDelay + DayOfWeek + DayofMonth + Quarter + OriginState)
form3 <- formula(ArrDelay ~ DepDelay + Distance + DayofMonth + Quarter + OriginState)
form4 <- formula(ArrDelay ~ DepDelay + Distance + DayOfWeek + Quarter)
  
bm_coef <- function(form, fc_var) {coef(biglm.big.matrix(form, flights_bm, fc = fc_var))["DepDelay"]}
  
forms <- list(form1, form2, form3, form4)
fc_vars <- list("OriginState", "OriginState", "OriginState", NULL)
```

Based on the defined `bm_coef()` and the respective lists, `mapply()` is used to sequentially compute the regression models.

```{r 2_seqBM3, echo=TRUE}
system.time({
  Rprof("Rprof_table_seqBM.out", memory.profiling = TRUE)
  res_bm_seq <- mapply(bm_coef, form = forms, fc_var = fc_vars)
  Rprof(NULL)
})
summary_out <- summaryRprof("Rprof_table_seqBM.out", memory = "tseries", diff = FALSE)
summary_out$mem.used <- (summary_out$vsize.small + summary_out$vsize.large) / 1024
max(summary_out$mem.used) - min(summary_out$mem.used)
```

## 4. Parallel computing with shared memory

Finally, the operation is conducted using both parallel computing and the `{bigmemory}` framework. This offers the advantage of making use of shared memory as explained in Kane, Emerson and Weston (2013). The implementation is straightforward as the usual `{foreach}` workflow is used with the input variables (**forms** and **fc_vars**). In addition, each worker can easily access the filebacked matrix by calling `bigmemory::attach.big.matrix()`.

```{r 2_parBM, echo=TRUE}
bm_fe_coef <- function(form, dataset, fc_var) {coef(biglm.big.matrix(form, dataset, fc = fc_var))["DepDelay"]}
xdesc <- describe(flights_bm)
registerDoParallel(detectCores())
system.time({
  res_bm_fe <- foreach(f = forms, fc = fc_vars, .combine = c, .packages = "bigmemory") %dopar% {
    d <- attach.big.matrix(xdesc)
    bm_fe_coef(f, d, fc)
  }
})
stopImplicitCluster()
```


```{r 2_parBM2, eval=FALSE, include=TRUE}
registerDoParallel(detectCores())

Rprof("Rprof_table_parBM.out", memory.profiling = TRUE)
res_bm_fe <- foreach(f = forms, fc = fc_vars, .combine = c) %dopar% {
  require("bigmemory")
  d <- attach.big.matrix(xdesc)
  bm_fe_coef(f, d, fc)
  }
Rprof(NULL)
summary_out <- summaryRprof("Rprof_table_parBM.out", memory = "tseries", diff = FALSE)
summary_out$mem.used <- (summary_out$vsize.small + summary_out$vsize.large) / 1024
max(summary_out$mem.used) - min(summary_out$mem.used)

stopImplicitCluster()

# run in Markdown: 309.8 kB
```

## Comparison
> Describe and explain the result of the comparison.

First, it is ensured that all of the conducted approaches work the same way by comparing the returned coefficients.

```{r 2_comp, echo=TRUE}
print(res_RAM_seq)
print(unlist(res_RAM_fe))
print(res_bm_seq)
print(res_bm_fe)
```

Hence, it can be seen that the computation of the coefficients is robust to the different specifications. Therefore, a comparison in terms of time and memory consumption is reasonable.

*Note: the exact figures might slightly deviate, the presented ones are from a previous run. Also, the respective memory consumption was monitored through the Activity Monitor which showed the parallel nature of several R session. This also displayed that the approach with shared memory entailed the lowest memory consumption in a process, followed by the parallel-RAM approach and lastly the sequential ones.*

Approach | Time | Memory consumption
-------------------- | ------------- | -------------
Sequential computing from RAM (`lm()`) | 28.19 sec. | 2,861,281 kB
Parallel computing from RAM | 15.5 sec. | 854.5 kB
Sequential computing with a `big.matrix` | 28.26 sec. | 1,108,343 kB
Parallel computing with shared memory | **10.62 sec.** | **309.8 kB**

Based on the table summarizing the time and memory consumption, it can be seen that both parallel implementations exceed the sequential ones in both dimensions. This highlights the benefit of using multiple cores to process the regressions in parallel. Furthermore, parallel computing with shared memory performs a lot better than the RAM counterpart. This is because using shared memory reduces overhead as multiple R sessions (workers) can reference the respective data structures (`big.matrix()`) which, in turn, reduces overhead.


*Note: it might still be that the monitoring of the RAM only captures the master process and not includes all workers. This is because RAM consumption of all the parallel execution is usually higher than the sequential ones. The main advantage of parallel computing is the processing time (as can be seen above) and not necessarily the handling of RAM. However, performing the operation with shared-memory should still result in significantly less overhead compared to the "standard" parallel way due to the discussed advantages.*


# Exercise 3: Using parallel programming to optimize a real-world application
> Speed up the computations (optimize the correlation of two matrices that are parts of arrays)

In this exercise, parallel programming and associated concepts are used to optimize a real-world problem, i.e. to efficiently compute the similarity (correlation) between parts of an RGB image. This is done for randomly created images with different sizes (small, medium, large) and the optimized versions are compared to a baseline approach for a differing number of used cores. Here, parallel programming is applicable as the different parts of the image are analyzed independently of each other and could thereby benefit from running on different workers simultaneously.

## Setup

First, the arrays (images) of differing sizes are defined. To ensure reproducibility, `set.seed()` is used.

```{r 3_arrays, echo=TRUE}
set.seed(42)
small <- array(runif(20 * 20 * 3, min = 0, max = 255), dim = c(20, 20, 3))
medium <- array(runif(500 * 500 * 3, min = 0, max = 255), dim = c(500, 500, 3)) 
large <- array(runif(1500 * 1500 * 3, min = 0, max = 255), dim = c(1500, 1500, 3))
```

Next, the base function `f()` is defined which measures the similarity by computing the correlation of the two halves of a matrix. This function stays unchanged and is used subsequently as the basis for optimization, as it already efficiently implemented.

```{r 3_f, echo=TRUE}
f <- function(mat) {
  ncol <- dim(mat)[2] # number of columns (i.e., matrix width)
  left <- mat[, 1:floor(ncol / 2)] # cut matrix into two equal pieces
  right <- mat[, ncol:(1 + ceiling(ncol / 2))] # reversed column order 
  vec_left <- as.vector(left) # convert matrices to vectors
  vec_right <- as.vector(right)
  corrLR <- stats::cor(vec_left, vec_right) # correlate vectors
  return(abs(corrLR)) # return absolute correlation
}
```

Furthermore, the data frames containing the timing results for the different approaches with respect to the number of used cores are initialized.

```{r 3_results_mat, echo=TRUE}
timing_results_small <- data.frame(cluster_size = 1:8)
timing_results_medium <- data.frame(cluster_size = 1:8)
timing_results_large <- data.frame(cluster_size = 1:8)
```

## Baseline function

First, the baseline function as given in the instructions is implemented. As this presents an sequential approach, using more than one core has no impact on the performance and therefore, the function is only executed ones per image.

*Note regarding the progress bar: here, it measures the progress on the number of cores that are analyzed. When one wold implement this in the image package, one would of course monitor something else such as the total number of matrices left (as one would only have one image and one number of cores). However, the number of cores is for this application most useful and also not entials with too many progress bars (3 arrays x 8 cores times = 24 progress bars)*

```{r 3_basic, echo=TRUE}
baseline_mat <- function(d) {   
  res <- vector(mode = "double", length = dim(d)[3])
  pb <- txtProgressBar(min = 1, max = dim(d)[3], style = 3)

  # 1. loop: the three color channels (i.e., third array dimension)
  for (color in seq_len(dim(d)[3])) {
    m <- d[, , color] # extract matrix
    m_width <- dim(m)[2] # compute width of matrix (i.e., number of columns) 
    m_range <- 0:floor(m_width * .05) # data shift range parameter
    # 2. loop: select and iterate over parts of the matrix 
    color_result_left <- vector(mode = "double", length(m_range)) 
    color_result_right <- vector(mode = "double", length(m_range)) 
    for (part in m_range) {
      # extract matrix parts
      m_part_left_shift <- m[, 1:(m_width - part)] 
      m_part_right_shift <- m[, (1 + part):m_width]
      # compute the similarity of respective matrices 
      color_result_left[part + 1] <- f(m_part_left_shift) 
      color_result_right[part + 1] <- f(m_part_right_shift)
      }
    # store max similarity for this color channel
    res[color] <- max(color_result_left, color_result_right) 
    setTxtProgressBar(pb, color)
  }
  return(res)
}


baseline <- function(d) {   
  start <- Sys.time()
  result_baseline <- baseline_mat(d)
  times <- as.numeric(Sys.time() - start)
  return(list("time" = times, "res" = result_baseline))
}


results_baseline_s <- baseline(small)
timing_results_small$baseline <- results_baseline_s$time

results_baseline_m <- baseline(medium)
timing_results_medium$baseline <- results_baseline_m$time

results_baseline_l <- baseline(large)
timing_results_large$baseline <- results_baseline_l$time
```

## `{foreach}`: Parallelize color matrices

In the first optimization approach, the three different color matrices are analyzed in parallel to make use of more than one core. This is implemented by defining a new function `corr_mat()` which executes the computation for a given matrix and color. Then, this function is executed in parallel using `foreach::foreach()` per different color.

Finally, it is validated that the results are exactly the same as the ones of the initial baseline approach for all three images. This will be implemented for each of the subsequently proposed methods.

```{r 3_foreach1, echo=TRUE}
corr_mat <- function(mat, color) {   
  m <- mat[, , color]
  m_width <- dim(m)[2]
  m_range <- 0:floor(m_width * .05)
  
  color_result_left <- vector(mode = "double", length(m_range)) 
  color_result_right <- vector(mode = "double", length(m_range)) 
  for (part in m_range) {
    m_part_left_shift <- m[, 1:(m_width - part)] 
    m_part_right_shift <- m[, (1 + part):m_width]
    # compute the similarity of respective matrices 
    color_result_left[part + 1] <- f(m_part_left_shift) 
    color_result_right[part + 1] <- f(m_part_right_shift)
    }
  # store max similarity for this color channel
  return(max(color_result_left, color_result_right)) 
  }


foreach1 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)
  for (core in 1:detectCores()) {
    registerDoParallel(core)
    start <- Sys.time()
    result_foreach1 <- foreach(color = seq_len(dim(d)[3]), .combine = c) %dopar% corr_mat(d, color)
    times[core] <- as.numeric(Sys.time() - start)
    stopImplicitCluster()
    setTxtProgressBar(pb, core)
  }
  return(list("time" = times, "res" = result_foreach1))
}


results_foreach1_s <- foreach1(small) 
timing_results_small$foreach1 <- results_foreach1_s$time

results_foreach1_m <- foreach1(medium)
timing_results_medium$foreach1 <- results_foreach1_m$time

results_foreach1_l <- foreach1(large)
timing_results_large$foreach1 <- results_foreach1_l$time

print(identical(results_baseline_s$res, results_foreach1_s$res))
print(identical(results_baseline_m$res, results_foreach1_m$res))
print(identical(results_baseline_l$res, results_foreach1_l$res))
```


## `{future}`: Parallelize color matrices

Similar to the previous two approaches, `future_apply::future_sapply()` is used to parallelize the computation of similarity per color. As before, it is validated that the identical results are obtained.

```{r 3_future1, echo=TRUE}
future1 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)
  
  for (core in 1:detectCores()) {
    plan(multisession, workers = core)
    start <- Sys.time()
    res_future1 <- future_sapply(seq_len(dim(d)[3]), corr_mat, mat = d)
    times[core] <- as.numeric(Sys.time() - start)
    plan(sequential)
    setTxtProgressBar(pb, core)
  }
  return(list("time" = times, "res" = res_future1))
}

  
results_future1_s <- future1(small)
timing_results_small$future1 <- results_future1_s$time

results_future1_m <- future1(medium)
timing_results_medium$future1 <- results_future1_m$time

results_future1_l <- future1(large)
timing_results_large$future1 <- results_future1_l$time

print(identical(results_baseline_s$res, results_future1_s$res))
print(identical(results_baseline_m$res, results_future1_m$res))
print(identical(results_baseline_l$res, results_future1_l$res))
```

## `parallel::mclapply()`: Parallelize color matrices 

The same workflow is implemented using the `{parallel}` package. Here, `mclapply()` is chosen over `parSapply()` because it performed better in exercise one and offers the advantage of forking. However, `parSapply()` could be easily used to perform this operation as well.

```{r 3_parallel2, echo=TRUE}
parallel2 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)
  
  for (core in 1:detectCores()) {
    start <- Sys.time()
    result_parallel2 <- mclapply(seq_len(dim(d)[3]), FUN = corr_mat, mat = d, mc.cores = core)
    times[core] <- as.numeric(Sys.time() - start)
    setTxtProgressBar(pb, core)
  }
  return(list("time" = times, "res" = result_parallel2))
}


results_parallel2_s <- parallel2(small)
timing_results_small$parallel2 <- results_parallel2_s$time

results_parallel2_m <- parallel2(medium)
timing_results_medium$parallel2 <- results_parallel2_m$time

results_parallel2_l <- parallel2(large)
timing_results_large$parallel2 <- results_parallel2_l$time

print(identical(results_baseline_s$res, unlist(results_parallel2_s$res)))
print(identical(results_baseline_m$res, unlist(results_parallel2_m$res)))
print(identical(results_baseline_l$res, unlist(results_parallel2_l$res)))
```

## `{foreach}`: Nested approach of colors & individual matrices

The presented baseline approaches of the parallel frameworks can split up the task into three separate problems and then solve these in parallel. Thereby, however, they only make use of three out of eight (in the case of this local machine) cores as the outer loop has only very few iterations. Hence, it might be more fruitful to further parallelize the computation of the submatrices of the individual colors to gain improvements in speed. To achieve this, a nested for loop is constructed which first splits the task by color and then by the different ranges as in the baseline method. Hence, `corr_mat2()` is applied which only needs the respective *part* parameter to return the respective correlation. Then, the individual results are aggregated using a custom combine function to only keep the maximum value, as it is the case in the initially stated baseline method.

By implementing this method, all available cores can be used.  

```{r 3_foreach2, echo=TRUE}
corr_mat2 <- function(m, part) {
  width <- dim(m)[2]
  m_part_left_shift <- m[, 1:(width - part)] 
  m_part_right_shift <- m[, (1 + part):width]
  return(max(f(m_part_left_shift), f(m_part_right_shift) )) 
}

combine_custom <- function(L1, L2) {
  return(max(L1, L2))
}


foreach2 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
   pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)
  
  for (core in 1:detectCores()) {
    registerDoParallel(core)
    start <- Sys.time()
    result_foreach2 <- foreach(color = seq_len(dim(d)[3]), .combine = c) %:% foreach(range = 0:floor(dim(d[, , color])[2] * .05), .combine = combine_custom) %dopar% corr_mat2(d[, , color], range)
    times[core] <- as.numeric(Sys.time() - start)
    stopImplicitCluster()
    setTxtProgressBar(pb, core)
  }
  return(list("time" = times, "res" = result_foreach2))
}


results_foreach2_s <- foreach2(small)
timing_results_small$foreach2 <- results_foreach2_s$time

results_foreach2_m <- foreach2(medium)
timing_results_medium$foreach2 <- results_foreach2_m$time

results_foreach2_l <- foreach2(large)
timing_results_large$foreach2 <- results_foreach2_l$time

print(identical(results_baseline_s$res, results_foreach2_s$res))
print(identical(results_baseline_m$res, results_foreach2_m$res))
print(identical(results_baseline_l$res, results_foreach2_l$res))
```

## `{foreach}`: Parallelize inidvidual matrices with load balancing and chunking

Next, the baseline approach is taken apart even further. More specifically, only the base function `f()` is now applied to individual matrices which are controlled by the concepts of load balancing and chunking with `{foreach}`. For that, the indices for the individual matrices are calculated once and stored in the list **l**. As these indices do not vary by color, is would be inefficient to determine them repetitively. Then, it is iterated through all the colors and for each of them, the number of total matrices is split into as many parts as cores are present using `itertools::isplitVector()`. Then, each core iteratively evaluates the matrices using the base `f()` function and returns the maximum value. Also, load balancing is set by specifying the argument `.options.multicore = list(preschedule = FALSE)`.

By implementing chunking, communication overhead might be reduced as individual cores directly process multiple matrices and only report the maximum similarity back to the master. Likewise, load balancing can lead to a gain in computation time as the computations of correlations might not be equally difficult and therefore, the next task can be sent as soon as a worker is done.

```{r 3_foreach3, echo=TRUE}
foreach3 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  mcoptions <- list(preschedule = FALSE)
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)

  for (core in 1:detectCores()) {
    registerDoParallel(core)
    l <- unique(as.list(sapply(0:floor(dim(d[, , 1])[2] * .05), 
                               function(part) {return(list("l" = 1:(dim(d)[2] - part), "r" = (1+part):(dim(d)[2])))})))
    
    start <- Sys.time()
    result_foreach3 <- foreach(color = seq_len(dim(d)[3]), .combine = c) %:% foreach(range = isplitVector(l, chunks = core), .combine = combine_custom, .options.multicore = mcoptions) %dopar% { return(max(sapply(range, function(x) {f(d[, x, color])}))) }
    times[core] <- as.numeric(Sys.time() - start)
    stopImplicitCluster()
    setTxtProgressBar(pb, core)
    }
  return(list("time" = times, "res" = result_foreach3))
}


results_foreach3_s <- foreach3(small)
timing_results_small$foreach3 <- results_foreach3_s$time

results_foreach3_m <- foreach3(medium)
timing_results_medium$foreach3 <- results_foreach3_m$time

results_foreach3_l <- foreach3(large)
timing_results_large$foreach3 <- results_foreach3_l$time

print(identical(results_baseline_s$res, results_foreach3_s$res))
print(identical(results_baseline_m$res, results_foreach3_m$res))
print(identical(results_baseline_l$res, results_foreach3_l$res))
```


## `{future}`: Parallelize inidvidual matrices with chunking

Using a similar idea as the previous approach, `{future}` is used to apply chunking when analyzing the separate submatrices. This is implemented using `future_apply::future_sapply()` with the options `future.chunk.size` which is set accordingly such that each available core receives the same number of matrices.

```{r 3_future2, echo=TRUE}
future2 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  mcoptions <- list(preschedule = FALSE)
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)

  for (core in 1:detectCores()) {

    l <- unique(as.list(sapply(0:floor(dim(d[, , 1])[2] * .05), 
                               function(part) {return(list("l" = 1:(dim(d)[2] - part), "r" = (1+part):(dim(d)[2])))})))
    
    result_foreach3 <- vector(mode = "double", length = dim(d)[3])
    plan(multisession, workers = core)
    start <- Sys.time()
    for (color in 1:dim(d)[3]) {
      result_foreach3[color] <- max(future_sapply(l, function(x) {f(d[, x, color])}, future.chunk.size = length(l)/core))
      } 
    times[core] <- as.numeric(Sys.time() - start)
    plan(sequential)
    setTxtProgressBar(pb, core)
    }
  return(list("time" = times, "res" = result_foreach3))
}


results_future2_s <- future2(small)
timing_results_small$future2 <- results_future2_s$time

results_future2_m <- future2(medium)
timing_results_medium$future2 <- results_future2_m$time

results_future2_l <- future2(large)
timing_results_large$future2 <- results_future2_l$time

print(identical(results_baseline_s$res, results_future2_s$res))
print(identical(results_baseline_m$res, results_future2_m$res))
print(identical(results_baseline_l$res, results_future2_l$res))
```

## `{parallel}`: Parallelize inidvidual matrices with load balancing

Finally, the concept of load balancing can also be easily implemented using `parallel::clusterApplyLB()`. Apart from that, this approach also determines the indices for the different submatrices and then iterates through the three color matrices to compute the maximum similarity. Note that load balancing could also be achieved using `parallel::mclapply()` and its option `mc.preschedule = FALSE`.

```{r 3_parallel3, echo=TRUE}
parallel3 <- function(d) {   
  times <- vector(mode = "double", length = detectCores())
  pb <- txtProgressBar(min = 1, max = detectCores(), style = 3)

  for (core in 1:detectCores()) {
    l <- unique(as.list(sapply(0:floor(dim(d[, , 1])[2] * .05), 
                               function(part) {return(list("l" = 1:(dim(d)[2] - part), "r" = (1+part):(dim(d)[2])))})))
    
    result_parallel3 <- vector(mode = "double", length = dim(d)[3])
    cls <- makeCluster(core)
    clusterExport(cls, "f")
    start <- Sys.time()
    for (color in 1:dim(d)[3]) {
      result_parallel3[color] <- max(unlist(clusterApplyLB(cls, l, function(x) {f(d[, x, color])})))
      }
    times[core] <- as.numeric(Sys.time() - start)
    stopCluster(cls)
    setTxtProgressBar(pb, core)
  }
  return(list("time" = times, "res" = result_parallel3))
}


results_parallel3_s <- parallel3(small)
timing_results_small$parallel3 <- results_parallel3_s$time

results_parallel3_m <- parallel3(medium)
timing_results_medium$parallel3 <- results_parallel3_m$time

results_parallel3_l <- parallel3(large)
timing_results_large$parallel3 <- results_parallel3_l$time

print(identical(results_baseline_s$res, unlist(results_parallel3_s$res)))
print(identical(results_baseline_m$res, unlist(results_parallel3_m$res)))
print(identical(results_baseline_l$res, unlist(results_parallel3_l$res)))
```

## Visual comparison of methods

Based on the previously executed approaches, it can be observed that all of them returns results identical to the results of the initial code. Therefore, it is reasonable to compare the computation times. This is implemented in the following by plotting the execution times against the used number of cores for the three arrays under consideration.

```{r 3_plot_s, echo=TRUE}
ggplot(timing_results_small,
     aes(x = cluster_size)) +
  geom_line(aes(y = baseline, color = "Baseline")) +
  geom_line(aes(y = foreach1, color = "Foreach: parall. col.")) +
  geom_line(aes(y = future1, color = "Future: parall. col.")) +
  geom_line(aes(y = foreach2, color = "Foreach: nested")) +
  geom_line(aes(y = parallel2, color = "mclapply: parall. col.")) +
  geom_line(aes(y = foreach3, color = "Foreach: f() + l.b + c.")) +
  geom_line(aes(y = future2, color = "Future: f() + c.")) +
  geom_line(aes(y = parallel3, color = "Parallel: f() + l.b")) +
  labs(y = "execution time (in sec.)",  x = "cluster_size") + 
  scale_color_brewer(name = "Methods", palette = "Dark2") +
  theme(legend.position = "right") +
  ggtitle("Small (20x20)")
```


```{r 3_plot_m, echo=TRUE}
ggplot(timing_results_medium,
     aes(x = cluster_size)) +
  geom_line(aes(y = baseline, color = "Baseline")) +
  geom_line(aes(y = foreach1, color = "Foreach: parall. col.")) +
  geom_line(aes(y = future1, color = "Future: parall. col.")) +
  geom_line(aes(y = foreach2, color = "Foreach: nested")) +
  geom_line(aes(y = parallel2, color = "mclapply: parall. col.")) +
  geom_line(aes(y = foreach3, color = "Foreach: f() + l.b + c.")) +
  geom_line(aes(y = future2, color = "Future: f() + c.")) +
  geom_line(aes(y = parallel3, color = "Parallel: f() + l.b")) +
  labs(y = "execution time (in sec.)",  x = "cluster_size") + 
  scale_color_brewer(name = "Methods", palette = "Dark2") +
  theme(legend.position = "right") +
  ggtitle("Medium (500x500)")
```

```{r 3_plot_l, echo=TRUE}
ggplot(timing_results_large,
     aes(x = cluster_size)) +
  geom_line(aes(y = baseline, color = "Baseline")) +
  geom_line(aes(y = foreach1, color = "Foreach: parall. col.")) +
  geom_line(aes(y = future1, color = "Future: parall. col.")) +
  geom_line(aes(y = foreach2, color = "Foreach: nested")) +
  geom_line(aes(y = parallel2, color = "mclapply: parall. col.")) +
  geom_line(aes(y = foreach3, color = "Foreach: f() + l.b + c.")) +
  geom_line(aes(y = future2, color = "Future: f() + c.")) +
  geom_line(aes(y = parallel3, color = "Parallel: f() + l.b")) +
  labs(y = "execution time (in sec.)",  x = "cluster_size") + 
  scale_color_brewer(name = "Methods", palette = "Dark2") +
  theme(legend.position = "right") +
  ggtitle("Large (1500x1500)")
```

Comparing the different graphs, it can be seen that the applicability of methods also depends to a certain degree on the size of the array. While the benefit of sophisticated parallel approaches is not present for the small array, it pays of for the medium and the large image. Here, it can also be seen that the more complex methods using chunking (*c*), load balancing (*l.b.*) and the individual matrices (*f()*) outperform the base approaches which simply parallelize the colors and therefore can only make use of three cores. Also, one could argue that the `{foreach}` method which parallelizes the individual matrices with load balancing and chunking could be proposed as it performs well for all three sizes and is also scalable for different hardware setups.

------------------------------------------------------------

Kane, Michael J., Emerson, John, and Stephen Weston (2013). Scalable Strategies for Computing with Massive Data. Journal of Statistical Software, 55(14), 1-19.

